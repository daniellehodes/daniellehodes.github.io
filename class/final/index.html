<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <!--seo-->
    <title>Title tag</title>
    <meta name="description" content="Tiktok-Algorithm.">
    <!--fonts-->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@200;300;400;500;600&family=Nunito:wght@300;400;500;600;700&&family=Open+Sans:wght@300;400&family=Sofia+Sans+Semi+Condensed:wght@100;200;300;400;700&display=swap"
        rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Cabin+Condensed:wght@400;500;600;700&family=Open+Sans:wght@300;500;600&display=swap"
        rel="stylesheet">
    <!-- bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous">
    <link rel="stylesheet" href="css/final.css">
</head>

<body>
    <div class="container">
        <header>
            <div class="row">
                <div class="col-md-7">
                    <h1>TikTok bombards teens<br> with harmful content<br> every 39 seconds</h1>
                </div>
                <div class="col-md-5">
                    <img src="images/tiktok-cry.png" class="img-fluid" alt="tiktok-crying">
                </div>
            </div>
        </header>
        <main>
            <div class="content">
                <div class="row ">
                    <p>Tiktok, the social media app that has swallowed the internet, is pushing eating disorder
                        and
                        self-harm content to 13-year-old users within 30 minutes of them joining the platform, research
                        shows.
                    </p>
                    <p>Two thirds of American teens use TikTok on average for 90 minutes a day. That time is spent
                        aimlessly
                        scrolling through an infinite stream of content, delivered in bite-sized videos catered to its
                        user's
                        interests. Bidding for people’s emotions, attention and screen time, TikTok out rules instagram,
                        facebook and youtube in stickiness.</p>
                    <p>TikTok has mastered its algorithmic recommendations, and within seconds the app learns a user's
                        likes and
                        dislikes. The ‘For You’ page is an endlessly scrollable carousel of videos selected based on the
                        videos
                        a user most interacts with. </p>
                    <p>The ‘For You’ page is “central to the TikTok experience and where most of our users spend their
                        time,”
                        stated the creator.</p>
                    <p>While the ‘For You’ feed creates a personalized user experience, it also introduces unique
                        dangers
                        and
                        can recommend harmful content. The same algorithm that pushes silly dance videos also suggests
                        mental
                        health, eating disorder or self-harm videos to its most vulnerable users, desperate to keep them
                        viewing
                        the content and ads that generate revenue. </p>
                    <p>“TikTok’s algorithm is the social media equivalent of crack cocaine: it’s refined, highly
                        addictive
                        and
                        leaves a trail of damage in its wake that its producers do not appear to care about” said Imran
                        Ahmed,
                        founder and CEO of Countering Digital Hate.</p>
                    <p>Researchers set up eight TikTok accounts in the United States, United Kingdom, Canada and
                        Australia
                        for
                        13 year olds, the minimum age the platform allows. Four of the TikTok accounts in the study were
                        classified as "Standard Teen Accounts" that spent time liking, pausing and
                        commenting on videos pertaining to body image and mental health. The results were alarming.</p>
                    <p>Every 39 seconds, the account was bombarded with content about mental-health and body-image.</p>
                    <p>Repeated exposure to body-image and mental-health related content is damaging to a teens mental
                        health,
                        even if the videos are not explicitly promoting eating disorders or self harm.</p>
                </div>
                <div class="row img">
                    <img src="images/mental-health-standard-teen.png" alt="standard-teen-mental-health-content">
                </div>
                <div class="row img">
                    <img src="images/standard-categories.png" alt="standard-teen-categroies-of-suggested-content">
                </div>
                <div class="row">
                    <p>The content pertaining to mental-health primarily spoke about anxieties and insecurities.</p>
                    <p>Videos
                        pertaining to body image recommended weight loss drinks and “tummy tuck” surgeries. One post
                        prescribed
                        the “boiled egg diet” of exclusively eating boiled eggs for two weeks to lose 24 lbs. The video
                        had
                        2,153 likes.</p>
                    <p>“The pathways into extreme content were so innocuous,”said Ahmed in an interview. “Your eye might
                        be
                        caught by a video of an aspirational body in beautiful clothes and very quickly the algorithm
                        realizes
                        you’re interested in body image.”</p>
                </div>
                <div class="row img">
                    <img src="images/egg-diet.png" alt="boiled-egg-diet">
                </div>
                <div class="row">
                    <p>Content about eating disorders or self harm surfaced very quickly.</p>
                    <p>Within 2.6 minutes, TikTok recommended suicide content. In 8 minutes, eating disorder content was
                        recommended.</p>
                    <p>While eating disorder and self harm content appeared at a lower rate than videos about body image
                        and mental health, the platform is nontheless recommending videos about suicide to accounts with
                        the stated age of 13.</p>
                    <p>Impressionable teen users who are exposed to such content within minutes of joining the platform
                        can develop greater interest in them.</p>
                </div>
                <div class="row img">
                    <img src="images/standard-table-tiktok.png" alt="standard-teen-harmful-conten">
                </div>
                <div class="row img">
                    <img src="images/standard-eating-suicide.png" alt="standard-teen-suicide-ed-content">
                </div>
                <div class="row">
                    <p>The most extreme content was about “junkorexia,” the unofficial term used to describe
                        people who are
                        anorexic but only eat junk food and other videos detailing suicide attempts.</p>
                    <p>Pro-eating disorder videos employed coded hashtags to evade content moderation. Ed
                        Sheeran’s name was
                        manipulated to mask harmful videos under the hashtag #EdSheeranDisorder. Videos within
                        the hashtag pool encouraged eating disorders through accountability posts. For example,
                        ‘thinspo,’ short for 'thinspiration' motivated weight loss and
                        ‘pro-ana’videos are easily found within TikTok hashtags. Hashtags hosting eating
                        disorder content have been viewed a total of 13.2 billion times.</p>
                </div>
                <div class="row img">
                    <img src="images/junkorexia.png" alt="junkorexia">
                </div>
                <div class="row">
                    <p>Tiktok does not moderate its hashtag pools and positive, recovery videos can freely
                        mingle in the same space
                        as pro-eating disorder content. </p>
                    <p>Users who actively seek out harmful content on social media will incorporate language
                        surrounding eating
                        disorders in their usernames such as “anorexia.” Researchers created an additional four
                        accounts
                        classified as “Vulnerable Teen Accounts” containing the phrase “lose weight” in their
                        usernames.
                        The Vulnerable Teen Accounts followed the same methodology of liking and pausing on body
                        image, mental
                        health and eating disorder videos. Within 30 minutes, the account was shown self harm
                        and eating
                        disorder content every 66 seconds. Vulnerable Teen Accounts saw 12 times as many suicide
                        videos as the
                        Standard Teen Accounts.</p>
                </div>
                <div class="row img">
                    <img src="images/vulnerable-suicide.png" class="img-fluid" alt="vulnerable-teens">
                </div>
                <div class="row img">
                    <img src="images/vulnerable-graph.png" alt="vulnerable-graph">
                </div>
                <div class="row">
                    <p>Damaging social media content promoting unhealthy behaviors can result in real life
                        consequences. A
                        British court ruled in September 2022 that Instagram and other social media platforms
                        contributed to the
                        death of Molly Russell, a 14-year-old girl who committed suicide. She saved, liked and
                        shared 2,100
                        posts about depression, self- harm or suicide six months before she took her own life,
                        according to data
                        obtained by her parents.</p>
                    <p>While Meta claimed it has not researched the impact of suicidal content on young users,
                        former Meta
                        employee Frances Haugen blew the whistle on the Facebook Papers revealing the company
                        had extensive data
                        proving otherwise.</p>
                    <p>Internal documents in the Facebook files admitted “we make body image issues worse for
                        one in three teen
                        girls.”</p>
                    <p>TikTok has been slow and ineffective in its damage control. Only seven of the 56 hashtags
                        hosting eating
                        disorder content were removed. One month after the CCDH report was released, eating
                        disorder content
                        garnered an additional 1.6 billion views in January 2023.</p>
                    <p>Tiktok has since rolled out a new feature that allows users to refresh their ‘For You’
                        feed if their
                        recommendations are no longer relevant. The app is working to limit recommendations of
                        topics that could
                        negatively impact a user if viewed repeatedly.</p>
                    <div class="row safety">
                        <img src="images/tiktok safety.gif" class="img-fluid" alt="tiktok-safety-features">
                    </div>
                    <p>“We recognize that too much of anything, whether it's animals, fitness tips, or personal
                        well-being
                        journeys, doesn't fit with the diverse discovery experience we aim to create,” according
                        to a statement.</p>
                    <p>The app said it aims to remove content that glorifies self-injury and promote recovery
                        or educational
                        content, with limits on how often that content is recommended. </p>
                    <p>“We do not allow showing or promoting disordered eating or any dangerous weight loss
                        behaviors,”according
                        to the press release. </p>
                    <p>Additionally, users below the age of 18 are automatically subject to a 60-minute daily
                        screen time limit,
                        as of March 1. If the limit is reached, teens will be prompted to enter a passcode to
                        continue watching.
                        Yet any user can lie about their age on the platform, as was done by researchers in the
                        study.</p>
                    <p>Older users are also impacted by harmful content promoted on the platform but do not
                        receive the same
                        safety measures provided for minors. Eating disorders are most commonly developed at age
                        21, and suicide
                        is more prevelant from age 25 onwards. About 5% of adults self harm, although the
                        highest rates are
                        found in teens and college students.</p>
                    <p>TikTok requires more transparency over the algorithm that pushes detrimental content to
                        its young users
                        or why that content varies based on a young accounts username.
                        Counter Digital Hate aims to hold TikTok liable for its coding algorithm instead of
                        allowing the
                        platform to hide behind the shield of Section 230 and continue to put children and
                        adults at risk.</p>
                    <p>“The stakes are too high for TikTok to continue to do nothing, or for our politicians to
                        sit back and
                        fail to act. We need platforms and politicians to have parents’ backs, but right now
                        they’re putting
                        profits before people,” said the Counter Digital Hate CEO.</p>
                </div>
            </div>
        </main>
</body>
<footer>
    <div class="row copyright">
    <p>Copyright: <a class="nav-link" href="http://127.0.0.1:5501/index.html">Danielle Hodes</a></p>
    </div>
</footer>

</div>
<!--js-->


</html>